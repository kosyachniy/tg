{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocesing notebook.\n",
    "\n",
    "As our data is raw and unprocessed in any way.\n",
    "\n",
    "let\\`s sort out uninformative texts and do some preprocessing and preparations before we start to train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f8db979afe95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing_tools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing_tools'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "\n",
    "import preprocessing_tools as pt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from os import mkdir \n",
    "from shutil import move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4154\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "data_path = '../Data/raw_data'\n",
    "\n",
    "template = join(data_path,'*.txt')\n",
    "filenames = glob(template)\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let`\\s tokenize all texts and collect texts lenghts to see texts lengths distribution.\n",
    "texts_lens = []\n",
    "for name in tqdm(filenames):\n",
    "    with open(name, 'r') as f:\n",
    "        text = f.read()\n",
    "        tok_text = nltk.word_tokenize(text)\n",
    "        tok_text = pt.normalize(tok_text, tokenized=True)\n",
    "        texts_lens.append(len(tok_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#texts lengths distribution\n",
    "plt.hist(texts_lens, bins=[i * 100 for i in range(int(50000/100))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A lot of very short(<100 words) texts.\n",
    "\n",
    "Such a short texts are quite uninformative and only will consume memory\n",
    "while being very sparse rows of Term-doc matrix.\n",
    "\n",
    "So let\\`s put them aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting small texts\n",
    "texts_lens = np.array(texts_lens)\n",
    "small_texts_mask = texts_lens < 100\n",
    "small_texts_names = np.array(filenames)[small_texts_mask]\n",
    "\n",
    "#creating separate directory for them.\n",
    "small_texts_dir = join(data_path, 'small_texts')\n",
    "mkdir(small_texts_dir)\n",
    "\n",
    "#moving them down there\n",
    "for name in small_texts_names:\n",
    "    filename = name.split('/')[-1]\n",
    "    dst = join(small_texts_dir, filename)\n",
    "    move(name, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "In the begining, I tried to create my own normalization/preprocessing/tokenization module.\n",
    "\n",
    "As I read more and more documentation of different NLP modules I found everything what I needed to preprocess texts.\n",
    "\n",
    "So, It is really excessive to create your own preproc, unless your preproc is very specific.\n",
    "\n",
    "Still, I kept my own prepoc as example and as a legacy of texts lengths filtering code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords:\n",
    "\n",
    "We got few sometimes intersecting but not identical sets of stopwords.\n",
    "\n",
    "Let\\`s put them all toghether.\n",
    "\n",
    "#### Note:\n",
    "Some sets of this keywords are obtained after first LDA models were trained while tuning those models.\n",
    "\n",
    "Exaclty, it is a file named 'final_extra_stop_words.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numver of stopwords: 796\n"
     ]
    }
   ],
   "source": [
    "#loading stopwords from all the sources we got\n",
    "\n",
    "#loading text files with external stopwords\n",
    "#merging via set.update()\n",
    "stopwords_path = join('../Data/stopwords/raw_stopwords')\n",
    "stopwords_template = join(stopwords_path, '*.txt')\n",
    "stopwords_files = glob(stopwords_template)\n",
    "extra_stopwords = set()\n",
    "for name in stopwords_files:\n",
    "    with open(name, 'r') as f:\n",
    "        words = f.readlines()\n",
    "        words = [word.strip() for word in words]\n",
    "        extra_stopwords.update(set(words))\n",
    "\n",
    "#Also putting there nltk stopwords\n",
    "stopwords = set()\n",
    "stopwords.update(set(nltk_stopwords.words('english')))\n",
    "stopwords.update(set(nltk_stopwords.words('russian')))\n",
    "stopwords.update(extra_stopwords)\n",
    "\n",
    "#some euristically added stopwords\n",
    "custom_stopwords = set(['http', 'https', 'ru', 'com', 'vk',\n",
    "                         'привет', 'здравствуйте', 'например', 'репост'])\n",
    "\n",
    "#stopwords from final LDA tuning\n",
    "#comment this piece it if you haven`t got them.\n",
    "final_stopwords_path = join(stopwords_path, 'final_extra_stopwords.pkl')\n",
    "with open(final_stopwords_path, 'rb') as f:\n",
    "    stopwords_from_top = pickle.load(f)\n",
    "stopwords.update(set(stopwords_from_top))\n",
    "\n",
    "stopwords.update(custom_stopwords)\n",
    "stopwords = list(stopwords)\n",
    "print('Total numver of stopwords:', len(stopwords))\n",
    "\n",
    "#serializing all-merged stopwords set\n",
    "with open('../Data/stopwords/stopwords.pkl', 'wb') as f:\n",
    "    pickle.dump(stopwords, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/train split.\n",
    "Also I\\`ll split data on train and test in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3738 416\n"
     ]
    }
   ],
   "source": [
    "template = join(data_path,'*.txt')\n",
    "filenames = glob(template)\n",
    "train_names, test_names = train_test_split(filenames, test_size=0.1, random_state=666)\n",
    "print(len(train_names), len(test_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serializing splits\n",
    "with open('../Data/train_names.pkl', 'wb') as f:\n",
    "    pickle.dump(train_names, f)\n",
    "    \n",
    "with open('../Data/test_names.pkl', 'wb') as f:\n",
    "    pickle.dump(test_names, f)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
